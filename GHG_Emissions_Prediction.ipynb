{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32efb7ab",
   "metadata": {},
   "source": [
    "# Supply Chain Emissions Modeling Using Industry and Commodity Data (2010‚Äì2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1c6fc",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "You have annual supply chain emission data from 2010‚Äì2016 categorized into industries and commodities. The goal is to develop a regression model that can predict the Supply Chain Emission Factors with Margins based on descriptive and quality metrics (substance, unit, reliability, temporal/geographical/technological/data collection correlations, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae1c80",
   "metadata": {},
   "source": [
    "# üå± Greenhouse Gas Emission Prediction Project\n",
    "\n",
    "**Project Goal:**  \n",
    "To analyze and predict greenhouse gas (GHG) emissions from various U.S. industries and commodities using the official dataset from [data.gov](https://catalog.data.gov/dataset/supply-chain-greenhouse-gas-emission-factors-for-us-industries-and-commodities).\n",
    "\n",
    "**Source:**  \n",
    "[Supply Chain Greenhouse Gas Emission Factors](https://catalog.data.gov/dataset/supply-chain-greenhouse-gas-emission-factors-for-us-industries-and-commodities)\n",
    "\n",
    "**Tools:** Python, Pandas, Scikit-learn, Matplotlib, Seaborn  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc9ed5",
   "metadata": {},
   "source": [
    "## üìÇ Dataset Overview\n",
    "\n",
    "This dataset contains supply chain emission factors associated with various U.S. industries and commodities.\n",
    "\n",
    "**Key Columns:**\n",
    "- `Commodity Code`: Industry classification code\n",
    "- `Commodity Name`: Name of the industry/commodity\n",
    "- `Substance`: Type of greenhouse gas (CO2, methane, nitrous oxide, etc.)\n",
    "- `Unit`: Measurement units (e.g., kg/2018 USD, purchaser price)\n",
    "- `Supply Chain Emission Factors with Margins`: Target variable for prediction\n",
    "- `DQ ReliabilityScore`: Data quality reliability score\n",
    "- `DQ TemporalCorrelation`: Temporal correlation quality score\n",
    "- `DQ GeographicalCorrelation`: Geographical correlation quality score\n",
    "- `DQ TechnologicalCorrelation`: Technological correlation quality score\n",
    "- `DQ DataCollection`: Data collection quality score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409dc066",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing\n",
    "\n",
    "Steps:\n",
    "- Handle missing values\n",
    "- Convert units where needed\n",
    "- Encode categorical features\n",
    "- Normalize/scale numeric columns\n",
    "\n",
    "## ü§ñ Model Building & Evaluation\n",
    "\n",
    "We aim to predict `Supply Chain Emission Factors with Margins` using regression models.\n",
    "\n",
    "Models to try:\n",
    "- Linear Regression\n",
    "- Random Forest Regressor\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R¬≤ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2558455",
   "metadata": {},
   "source": [
    "##### Steps:\n",
    "- Step 1: Import Required Libraries\n",
    "- Step 2: Load Dataset\n",
    "- Step 3: Data Preprocessing (EDA+Cleaning+Encoding)\n",
    "- Step 4: Training\n",
    "- Step 5: Prediction and Evaluation\n",
    "- Step 6: Hyperparameter Tuning\n",
    "- Step 7: Comparative Study and Selecting the Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118424af",
   "metadata": {},
   "source": [
    "# Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e58624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670b39a",
   "metadata": {},
   "source": [
    "# Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7223e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Excel file path and years range\n",
    "excel_file = 'SupplyChainEmissionFactorsforUSIndustriesCommodities.xlsx'\n",
    "years = range(2010, 2017)\n",
    "\n",
    "print(f\"Years to process: {list(years)}\")\n",
    "print(f\"Looking for file: {excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_data(excel_file, years):\n",
    "    \"\"\"\n",
    "    Load data from Excel file with multiple sheets (one per year)\n",
    "    and combine them into a single DataFrame\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(excel_file):\n",
    "        print(f\"Error: File '{excel_file}' not found!\")\n",
    "        print(\"Please ensure the Excel file is in the current directory.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # First, let's see what sheets are available\n",
    "        xl_file = pd.ExcelFile(excel_file)\n",
    "        available_sheets = xl_file.sheet_names\n",
    "        print(f\"Available sheets in Excel file: {available_sheets}\")\n",
    "        \n",
    "        # Load data for each year\n",
    "        for year in years:\n",
    "            sheet_name = str(year)  # Assuming sheets are named by year\n",
    "            \n",
    "            if sheet_name in available_sheets:\n",
    "                print(f\"Loading data for year {year}...\")\n",
    "                df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "                df['Year'] = year  # Add year column\n",
    "                combined_data.append(df)\n",
    "                print(f\"  - Loaded {len(df)} rows for {year}\")\n",
    "            else:\n",
    "                print(f\"Warning: Sheet '{sheet_name}' not found in Excel file\")\n",
    "        \n",
    "        if combined_data:\n",
    "            # Combine all years into one DataFrame\n",
    "            final_df = pd.concat(combined_data, ignore_index=True)\n",
    "            print(f\"\\nTotal combined dataset shape: {final_df.shape}\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No data was loaded from any sheets.\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_and_combine_data(excel_file, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_fallback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Excel file is not available, create a sample structure for demonstration\n",
    "if df is None:\n",
    "    print(\"\\n‚ö†Ô∏è Excel file not found. Creating sample data structure for demonstration...\")\n",
    "    print(\"Please replace this with your actual data file.\")\n",
    "    \n",
    "    # Create sample data with the expected structure\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    sample_data = {\n",
    "        'Commodity Code': [f'{np.random.choice([\"1111\", \"2211\", \"3311\", \"4411\"])}{chr(65 + np.random.randint(0, 3))}0' for _ in range(n_samples)],\n",
    "        'Commodity Name': np.random.choice([\n",
    "            'Fresh soybeans, canola, flaxseeds, and other oilseeds',\n",
    "            'Fresh wheat, corn, rice, and other grains',\n",
    "            'Fresh fruits and tree nuts',\n",
    "            'Fresh vegetables, melons, and potatoes'\n",
    "        ], n_samples),\n",
    "        'Substance': np.random.choice(['carbon dioxide', 'methane', 'nitrous oxide', 'other GHGs'], n_samples),\n",
    "        'Unit': np.random.choice([\n",
    "            'kg/2018 USD, purchaser price',\n",
    "            'kg CO2e/2018 USD, purchaser price'\n",
    "        ], n_samples),\n",
    "        'Supply Chain Emission Factors without Margins': np.random.lognormal(0, 1, n_samples),\n",
    "        'Margins of Supply Chain Emission Factors': np.random.exponential(0.1, n_samples),\n",
    "        'Supply Chain Emission Factors with Margins': None,  # Will calculate\n",
    "        'DQ ReliabilityScore of Factors without Margins': np.random.randint(1, 6, n_samples),\n",
    "        'DQ TemporalCorrelation of Factors without Margins': np.random.randint(1, 6, n_samples),\n",
    "        'DQ GeographicalCorrelation of Factors without Margins': np.random.randint(1, 6, n_samples),\n",
    "        'DQ TechnologicalCorrelation of Factors without Margins': np.random.randint(1, 6, n_samples),\n",
    "        'DQ DataCollection of Factors without Margins': np.random.randint(1, 6, n_samples),\n",
    "        'Year': np.random.choice(list(years), n_samples)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    # Calculate target variable\n",
    "    df['Supply Chain Emission Factors with Margins'] = (\n",
    "        df['Supply Chain Emission Factors without Margins'] + \n",
    "        df['Margins of Supply Chain Emission Factors']\n",
    "    )\n",
    "    \n",
    "    print(f\"Sample dataset created with shape: {df.shape}\")\n",
    "    print(\"\\n‚ö†Ô∏è This is sample data for demonstration. Please replace with your actual Excel file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_basic_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "if df is not None:\n",
    "    print(\"üìä Dataset Basic Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_header",
   "metadata": {},
   "source": [
    "# Step 3: Data Preprocessing (EDA + Cleaning + Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_header",
   "metadata": {},
   "source": [
    "## 3.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Basic statistics\n",
    "    print(\"üìà Dataset Summary Statistics:\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    print(f\"Years covered: {sorted(df['Year'].unique())}\")\n",
    "    print(f\"Unique commodities: {df['Commodity Code'].nunique()}\")\n",
    "    print(f\"Unique substances: {df['Substance'].nunique()}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    print(\"\\nüîç Missing Values Analysis:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing Percentage': missing_percent\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "    if len(missing_df) > 0:\n",
    "        display(missing_df)\n",
    "    else:\n",
    "        print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_variable_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Target variable analysis\n",
    "    target_col = 'Supply Chain Emission Factors with Margins'\n",
    "    \n",
    "    print(f\"üéØ Target Variable Analysis: {target_col}\")\n",
    "    print(f\"Statistics:\")\n",
    "    print(df[target_col].describe())\n",
    "    \n",
    "    # Visualization of target variable\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(df[target_col], bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution of Target Variable')\n",
    "    axes[0, 0].set_xlabel(target_col)\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 1].boxplot(df[target_col])\n",
    "    axes[0, 1].set_title('Box Plot of Target Variable')\n",
    "    axes[0, 1].set_ylabel(target_col)\n",
    "    \n",
    "    # Log-scale histogram (for better visualization if data is skewed)\n",
    "    axes[1, 0].hist(np.log1p(df[target_col]), bins=50, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 0].set_title('Log-scale Distribution of Target Variable')\n",
    "    axes[1, 0].set_xlabel(f'log1p({target_col})')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Target by year\n",
    "    yearly_stats = df.groupby('Year')[target_col].agg(['mean', 'median', 'std']).reset_index()\n",
    "    axes[1, 1].plot(yearly_stats['Year'], yearly_stats['mean'], marker='o', label='Mean', linewidth=2)\n",
    "    axes[1, 1].plot(yearly_stats['Year'], yearly_stats['median'], marker='s', label='Median', linewidth=2)\n",
    "    axes[1, 1].set_title('Target Variable Trends by Year')\n",
    "    axes[1, 1].set_xlabel('Year')\n",
    "    axes[1, 1].set_ylabel(target_col)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorical_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Categorical variables analysis\n",
    "    categorical_cols = ['Substance', 'Unit']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Substance distribution\n",
    "    substance_counts = df['Substance'].value_counts()\n",
    "    axes[0, 0].pie(substance_counts.values, labels=substance_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Distribution of Substances')\n",
    "    \n",
    "    # Unit distribution\n",
    "    unit_counts = df['Unit'].value_counts()\n",
    "    axes[0, 1].pie(unit_counts.values, labels=unit_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Distribution of Units')\n",
    "    \n",
    "    # Target by Substance\n",
    "    substance_target = df.groupby('Substance')[target_col].mean().sort_values(ascending=True)\n",
    "    axes[1, 0].barh(range(len(substance_target)), substance_target.values, color='lightcoral')\n",
    "    axes[1, 0].set_yticks(range(len(substance_target)))\n",
    "    axes[1, 0].set_yticklabels(substance_target.index)\n",
    "    axes[1, 0].set_title('Average Target Value by Substance')\n",
    "    axes[1, 0].set_xlabel(f'Average {target_col}')\n",
    "    \n",
    "    # Data Quality Scores distribution\n",
    "    dq_cols = [col for col in df.columns if 'DQ' in col and 'Factors without Margins' in col]\n",
    "    if dq_cols:\n",
    "        dq_means = df[dq_cols].mean()\n",
    "        axes[1, 1].bar(range(len(dq_means)), dq_means.values, color='lightblue')\n",
    "        axes[1, 1].set_xticks(range(len(dq_means)))\n",
    "        axes[1, 1].set_xticklabels([col.replace('DQ ', '').replace(' of Factors without Margins', '') \n",
    "                                   for col in dq_means.index], rotation=45, ha='right')\n",
    "        axes[1, 1].set_title('Average Data Quality Scores')\n",
    "        axes[1, 1].set_ylabel('Average Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Correlation analysis\n",
    "    print(\"üîó Correlation Analysis\")\n",
    "    \n",
    "    # Select numeric columns for correlation\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlations with target variable\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    print(f\"\\nCorrelations with {target_col}:\")\n",
    "    for var, corr in target_correlations.items():\n",
    "        if var != target_col:\n",
    "            print(f\"{var}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning_header",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üßπ Data Cleaning Process\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove rows with missing target variable\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=[target_col])\n",
    "    print(f\"Removed {initial_rows - len(df_clean)} rows with missing target variable\")\n",
    "    \n",
    "    # 2. Handle outliers in target variable (using IQR method)\n",
    "    Q1 = df_clean[target_col].quantile(0.25)\n",
    "    Q3 = df_clean[target_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = (df_clean[target_col] < lower_bound) | (df_clean[target_col] > upper_bound)\n",
    "    outliers_count = outliers_mask.sum()\n",
    "    print(f\"Identified {outliers_count} outliers in target variable\")\n",
    "    \n",
    "    # Option to remove or cap outliers (here we'll cap them)\n",
    "    df_clean[target_col] = df_clean[target_col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    print(f\"Capped outliers to range [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "    \n",
    "    # 3. Handle missing values in other columns\n",
    "    # Fill missing values in DQ columns with median\n",
    "    dq_columns = [col for col in df_clean.columns if 'DQ' in col]\n",
    "    for col in dq_columns:\n",
    "        if df_clean[col].isnull().any():\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Filled missing values in {col} with median: {median_value}\")\n",
    "    \n",
    "    # 4. Remove any remaining rows with missing critical data\n",
    "    critical_cols = ['Commodity Code', 'Commodity Name', 'Substance', 'Unit']\n",
    "    before_critical = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=critical_cols)\n",
    "    print(f\"Removed {before_critical - len(df_clean)} rows with missing critical data\")\n",
    "    \n",
    "    print(f\"\\nFinal cleaned dataset shape: {df_clean.shape}\")\n",
    "    print(f\"Data cleaning completed successfully!\")\n",
    "    \n",
    "    # Update the main dataframe\n",
    "    df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering_header",
   "metadata": {},
   "source": [
    "## 3.3 Feature Engineering and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîß Feature Engineering and Encoding\")\n",
    "    \n",
    "    # Create a copy for feature engineering\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # 1. Create additional features\n",
    "    print(\"Creating additional features...\")\n",
    "    \n",
    "    # Calculate ratio features\n",
    "    if 'Supply Chain Emission Factors without Margins' in df_featured.columns and 'Margins of Supply Chain Emission Factors' in df_featured.columns:\n",
    "        df_featured['Margin_Ratio'] = (df_featured['Margins of Supply Chain Emission Factors'] / \n",
    "                                      (df_featured['Supply Chain Emission Factors without Margins'] + 1e-8))\n",
    "        print(\"  - Created Margin_Ratio feature\")\n",
    "    \n",
    "    # Create DQ composite score\n",
    "    dq_cols = [col for col in df_featured.columns if 'DQ' in col and 'Factors without Margins' in col]\n",
    "    if dq_cols:\n",
    "        df_featured['DQ_Composite_Score'] = df_featured[dq_cols].mean(axis=1)\n",
    "        df_featured['DQ_Score_Std'] = df_featured[dq_cols].std(axis=1)\n",
    "        print(f\"  - Created DQ_Composite_Score and DQ_Score_Std from {len(dq_cols)} DQ columns\")\n",
    "    \n",
    "    # Create commodity category based on first digit of commodity code\n",
    "    df_featured['Commodity_Category'] = df_featured['Commodity Code'].astype(str).str[0]\n",
    "    print(\"  - Created Commodity_Category feature\")\n",
    "    \n",
    "    # 2. Encode categorical variables\n",
    "    print(\"\\nEncoding categorical variables...\")\n",
    "    \n",
    "    # Initialize encoders dictionary\n",
    "    encoders = {}\n",
    "    \n",
    "    # Label encode categorical variables\n",
    "    categorical_columns = ['Substance', 'Unit', 'Commodity_Category']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df_featured.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_featured[f'{col}_encoded'] = le.fit_transform(df_featured[col].astype(str))\n",
    "            encoders[col] = le\n",
    "            print(f\"  - Label encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # One-hot encode substance (since it's an important categorical variable)\n",
    "    if 'Substance' in df_featured.columns:\n",
    "        substance_dummies = pd.get_dummies(df_featured['Substance'], prefix='Substance')\n",
    "        df_featured = pd.concat([df_featured, substance_dummies], axis=1)\n",
    "        print(f\"  - One-hot encoded Substance: {substance_dummies.shape[1]} dummy variables\")\n",
    "    \n",
    "    # 3. Select features for modeling\n",
    "    print(\"\\nSelecting features for modeling...\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    feature_columns = []\n",
    "    \n",
    "    # Add numeric features\n",
    "    numeric_features = ['Supply Chain Emission Factors without Margins', 'Margins of Supply Chain Emission Factors']\n",
    "    for col in numeric_features:\n",
    "        if col in df_featured.columns:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    # Add DQ features\n",
    "    dq_features = dq_cols + ['DQ_Composite_Score', 'DQ_Score_Std']\n",
    "    for col in dq_features:\n",
    "        if col in df_featured.columns:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    # Add encoded categorical features\n",
    "    encoded_features = [f'{col}_encoded' for col in categorical_columns]\n",
    "    for col in encoded_features:\n",
    "        if col in df_featured.columns:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    # Add substance dummy variables\n",
    "    substance_cols = [col for col in df_featured.columns if col.startswith('Substance_')]\n",
    "    feature_columns.extend(substance_cols)\n",
    "    \n",
    "    # Add year and engineered features\n",
    "    other_features = ['Year', 'Margin_Ratio']\n",
    "    for col in other_features:\n",
    "        if col in df_featured.columns:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    # Remove duplicates and ensure all columns exist\n",
    "    feature_columns = list(set(feature_columns))\n",
    "    feature_columns = [col for col in feature_columns if col in df_featured.columns]\n",
    "    \n",
    "    print(f\"Selected {len(feature_columns)} features for modeling:\")\n",
    "    for i, col in enumerate(feature_columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # Create final dataset for modeling\n",
    "    X = df_featured[feature_columns].copy()\n",
    "    y = df_featured[target_col].copy()\n",
    "    \n",
    "    print(f\"\\nFinal dataset shapes:\")\n",
    "    print(f\"  Features (X): {X.shape}\")\n",
    "    print(f\"  Target (y): {y.shape}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    df_final = df_featured.copy()\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature engineering completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_header",
   "metadata": {},
   "source": [
    "# Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in locals() and 'y' in locals():\n",
    "    print(\"üöÇ Preparing Data for Training\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=df_final['Year']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Check for any remaining missing values\n",
    "    print(f\"\\nMissing values in training set: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in test set: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Handle any remaining missing values\n",
    "    if X_train.isnull().sum().sum() > 0 or X_test.isnull().sum().sum() > 0:\n",
    "        print(\"Handling remaining missing values...\")\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train = pd.DataFrame(imputer.fit_transform(X_train), \n",
    "                              columns=X_train.columns, \n",
    "                              index=X_train.index)\n",
    "        X_test = pd.DataFrame(imputer.transform(X_test), \n",
    "                             columns=X_test.columns, \n",
    "                             index=X_test.index)\n",
    "        print(\"Missing values handled with median imputation\")\n",
    "    \n",
    "    # Scale the features\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrames for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    print(\"‚úÖ Data preparation completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Error: Features (X) and target (y) not available. Please run the preprocessing steps first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_scaled' in locals():\n",
    "    print(\"ü§ñ Training Machine Learning Models\")\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store trained models and results\n",
    "    trained_models = {}\n",
    "    model_results = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüìä Training {name}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        if name == 'Linear Regression':\n",
    "            # Use scaled data for Linear Regression\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred_train = model.predict(X_train_scaled)\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            # Use original data for Random Forest (doesn't require scaling)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Store results\n",
    "        model_results[name] = {\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'predictions_train': y_pred_train,\n",
    "            'predictions_test': y_pred_test\n",
    "        }\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Training RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"  Training MAE: {train_mae:.4f}\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  Training R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
    "        \n",
    "        # Calculate cross-validation scores\n",
    "        print(f\"\\n  Cross-validation (5-fold):\")\n",
    "        if name == 'Linear Regression':\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "        else:\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "        \n",
    "        print(f\"    CV R¬≤ scores: {cv_scores}\")\n",
    "        print(f\"    CV R¬≤ mean: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        model_results[name]['cv_r2_mean'] = cv_scores.mean()\n",
    "        model_results[name]['cv_r2_std'] = cv_scores.std()\n",
    "    \n",
    "    print(\"\\n‚úÖ Model training completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Error: Scaled training data not available. Please run the data preparation step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_header",
   "metadata": {},
   "source": [
    "# Step 5: Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model_results' in locals():\n",
    "    print(\"üìä Model Evaluation and Visualization\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for name, results in model_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Train RMSE': results['train_rmse'],\n",
    "            'Test RMSE': results['test_rmse'],\n",
    "            'Train MAE': results['train_mae'],\n",
    "            'Test MAE': results['test_mae'],\n",
    "            'Train R¬≤': results['train_r2'],\n",
    "            'Test R¬≤': results['test_r2'],\n",
    "            'CV R¬≤ Mean': results['cv_r2_mean'],\n",
    "            'CV R¬≤ Std': results['cv_r2_std']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nüìà Model Performance Comparison:\")\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Actual vs Predicted plots\n",
    "    for i, (name, results) in enumerate(model_results.items()):\n",
    "        ax = axes[0, i]\n",
    "        \n",
    "        # Plot actual vs predicted for test set\n",
    "        ax.scatter(y_test, results['predictions_test'], alpha=0.6, s=20)\n",
    "        \n",
    "        # Plot perfect prediction line\n",
    "        min_val = min(y_test.min(), results['predictions_test'].min())\n",
    "        max_val = max(y_test.max(), results['predictions_test'].max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "        \n",
    "        ax.set_xlabel(f'Actual {target_col}')\n",
    "        ax.set_ylabel(f'Predicted {target_col}')\n",
    "        ax.set_title(f'{name}\\nActual vs Predicted (Test Set)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add R¬≤ score to the plot\n",
    "        ax.text(0.05, 0.95, f'R¬≤ = {results[\"test_r2\"]:.3f}', \n",
    "                transform=ax.transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 2. Residual plots\n",
    "    for i, (name, results) in enumerate(model_results.items()):\n",
    "        ax = axes[1, i]\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_test - results['predictions_test']\n",
    "        \n",
    "        # Plot residuals\n",
    "        ax.scatter(results['predictions_test'], residuals, alpha=0.6, s=20)\n",
    "        ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel(f'Predicted {target_col}')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        ax.set_title(f'{name}\\nResidual Plot (Test Set)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add RMSE to the plot\n",
    "        ax.text(0.05, 0.95, f'RMSE = {results[\"test_rmse\"]:.3f}', \n",
    "                transform=ax.transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model performance comparison bar chart\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['Test RMSE', 'Test MAE', 'Test R¬≤']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = comparison_df[metric].values\n",
    "        models = comparison_df['Model'].values\n",
    "        \n",
    "        bars = axes[i].bar(models, values, alpha=0.7, \n",
    "                          color=['skyblue', 'lightcoral'])\n",
    "        axes[i].set_title(f'{metric} Comparison')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(values),\n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        axes[i].set_ylim(0, max(values) * 1.15)\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: Model results not available. Please run the model training step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trained_models' in locals():\n",
    "    print(\"üîç Feature Importance Analysis\")\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    if 'Random Forest' in trained_models:\n",
    "        rf_model = trained_models['Random Forest']\n",
    "        feature_importance = rf_model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nüå≤ Random Forest Feature Importance (Top 15):\")\n",
    "        display(importance_df.head(15).round(4))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "        \n",
    "        bars = plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "                       color='lightgreen', alpha=0.8)\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, importance) in enumerate(zip(bars, top_features['Importance'])):\n",
    "            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{importance:.3f}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Linear Regression Coefficients\n",
    "    if 'Linear Regression' in trained_models:\n",
    "        lr_model = trained_models['Linear Regression']\n",
    "        coefficients = lr_model.coef_\n",
    "        feature_names = X_train_scaled.columns\n",
    "        \n",
    "        # Create coefficients DataFrame\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': coefficients,\n",
    "            'Abs_Coefficient': np.abs(coefficients)\n",
    "        }).sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"\\nüìä Linear Regression Coefficients (Top 15 by Absolute Value):\")\n",
    "        display(coef_df[['Feature', 'Coefficient']].head(15).round(4))\n",
    "        \n",
    "        # Plot coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_coefs = coef_df.head(15)\n",
    "        \n",
    "        colors = ['red' if x < 0 else 'blue' for x in top_coefs['Coefficient']]\n",
    "        bars = plt.barh(range(len(top_coefs)), top_coefs['Coefficient'], \n",
    "                       color=colors, alpha=0.7)\n",
    "        plt.yticks(range(len(top_coefs)), top_coefs['Feature'])\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.title('Top 15 Linear Regression Coefficients (by Absolute Value)')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, coef) in enumerate(zip(bars, top_coefs['Coefficient'])):\n",
    "            x_pos = bar.get_width() + (0.01 if coef >= 0 else -0.01)\n",
    "            ha = 'left' if coef >= 0 else 'right'\n",
    "            plt.text(x_pos, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{coef:.3f}', va='center', ha=ha, fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Error: Trained models not available. Please run the model training step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_header",
   "metadata": {},
   "source": [
    "# Step 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"üîß Hyperparameter Tuning\")\n",
    "    \n",
    "    # Hyperparameter tuning for Random Forest\n",
    "    print(\"\\nüå≤ Tuning Random Forest Hyperparameters...\")\n",
    "    \n",
    "    # Define parameter grid for Random Forest\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    # Reduce parameter grid for faster execution (can be expanded for thorough tuning)\n",
    "    rf_param_grid_reduced = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['sqrt']\n",
    "    }\n",
    "    \n",
    "    # Grid search for Random Forest\n",
    "    rf_grid_search = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        rf_param_grid_reduced,\n",
    "        cv=3,  # Reduced for faster execution\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"  Starting Grid Search (this may take a few minutes...)\")\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"  Best Random Forest parameters: {rf_grid_search.best_params_}\")\n",
    "    print(f\"  Best cross-validation R¬≤ score: {rf_grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Get the best Random Forest model\n",
    "    best_rf = rf_grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the best Random Forest model\n",
    "    y_pred_train_best_rf = best_rf.predict(X_train)\n",
    "    y_pred_test_best_rf = best_rf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for best RF\n",
    "    best_rf_results = {\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_best_rf)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_best_rf)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train_best_rf),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test_best_rf),\n",
    "        'train_r2': r2_score(y_train, y_pred_train_best_rf),\n",
    "        'test_r2': r2_score(y_test, y_pred_test_best_rf),\n",
    "        'best_params': rf_grid_search.best_params_,\n",
    "        'cv_score': rf_grid_search.best_score_\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  Tuned Random Forest Performance:\")\n",
    "    print(f\"    Training RMSE: {best_rf_results['train_rmse']:.4f}\")\n",
    "    print(f\"    Test RMSE: {best_rf_results['test_rmse']:.4f}\")\n",
    "    print(f\"    Training R¬≤: {best_rf_results['train_r2']:.4f}\")\n",
    "    print(f\"    Test R¬≤: {best_rf_results['test_r2']:.4f}\")\n",
    "    \n",
    "    # Store the best model\n",
    "    trained_models['Random Forest (Tuned)'] = best_rf\n",
    "    model_results['Random Forest (Tuned)'] = best_rf_results\n",
    "    \n",
    "    print(\"\\n‚úÖ Hyperparameter tuning completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: Training data not available. Please run the data preparation steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_header",
   "metadata": {},
   "source": [
    "# Step 7: Comparative Study and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model_results' in locals():\n",
    "    print(\"üèÜ Final Model Comparison and Best Model Selection\")\n",
    "    \n",
    "    # Create comprehensive comparison\n",
    "    final_comparison = []\n",
    "    \n",
    "    for name, results in model_results.items():\n",
    "        comparison_entry = {\n",
    "            'Model': name,\n",
    "            'Test RMSE': results['test_rmse'],\n",
    "            'Test MAE': results['test_mae'],\n",
    "            'Test R¬≤': results['test_r2'],\n",
    "            'Train R¬≤': results['train_r2'],\n",
    "            'Overfitting (Train R¬≤ - Test R¬≤)': results['train_r2'] - results['test_r2']\n",
    "        }\n",
    "        \n",
    "        # Add CV score if available\n",
    "        if 'cv_r2_mean' in results:\n",
    "            comparison_entry['CV R¬≤ Mean'] = results['cv_r2_mean']\n",
    "            comparison_entry['CV R¬≤ Std'] = results['cv_r2_std']\n",
    "        elif 'cv_score' in results:\n",
    "            comparison_entry['CV R¬≤ Mean'] = results['cv_score']\n",
    "            comparison_entry['CV R¬≤ Std'] = 0.0  # From grid search, single score\n",
    "        \n",
    "        final_comparison.append(comparison_entry)\n",
    "    \n",
    "    final_comparison_df = pd.DataFrame(final_comparison)\n",
    "    \n",
    "    print(\"\\nüìä Final Model Performance Comparison:\")\n",
    "    display(final_comparison_df.round(4))\n",
    "    \n",
    "    # Determine the best model based on Test R¬≤ score\n",
    "    best_model_name = final_comparison_df.loc[final_comparison_df['Test R¬≤'].idxmax(), 'Model']\n",
    "    best_model_performance = final_comparison_df.loc[final_comparison_df['Test R¬≤'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "    print(f\"   Test R¬≤ Score: {best_model_performance['Test R¬≤']:.4f}\")\n",
    "    print(f\"   Test RMSE: {best_model_performance['Test RMSE']:.4f}\")\n",
    "    print(f\"   Test MAE: {best_model_performance['Test MAE']:.4f}\")\n",
    "    print(f\"   Overfitting Score: {best_model_performance['Overfitting (Train R¬≤ - Test R¬≤)']:.4f}\")\n",
    "    \n",
    "    # Model selection criteria explanation\n",
    "    print(\"\\nüìã Model Selection Criteria:\")\n",
    "    print(\"   1. Highest Test R¬≤ Score (primary criterion)\")\n",
    "    print(\"   2. Lowest Test RMSE (secondary criterion)\")\n",
    "    print(\"   3. Minimal overfitting (Train R¬≤ - Test R¬≤ should be small)\")\n",
    "    print(\"   4. Consistent cross-validation performance\")\n",
    "    \n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Test R¬≤ comparison\n",
    "    models = final_comparison_df['Model']\n",
    "    test_r2 = final_comparison_df['Test R¬≤']\n",
    "    \n",
    "    bars1 = axes[0, 0].bar(models, test_r2, alpha=0.8, \n",
    "                          color=['gold' if m == best_model_name else 'lightblue' for m in models])\n",
    "    axes[0, 0].set_title('Test R¬≤ Score Comparison')\n",
    "    axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, test_r2):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Test RMSE comparison\n",
    "    test_rmse = final_comparison_df['Test RMSE']\n",
    "    bars2 = axes[0, 1].bar(models, test_rmse, alpha=0.8,\n",
    "                          color=['gold' if m == best_model_name else 'lightcoral' for m in models])\n",
    "    axes[0, 1].set_title('Test RMSE Comparison')\n",
    "    axes[0, 1].set_ylabel('RMSE')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, test_rmse):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(test_rmse)*0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Overfitting analysis\n",
    "    overfitting = final_comparison_df['Overfitting (Train R¬≤ - Test R¬≤)']\n",
    "    bars3 = axes[1, 0].bar(models, overfitting, alpha=0.8,\n",
    "                          color=['red' if x > 0.1 else 'green' for x in overfitting])\n",
    "    axes[1, 0].set_title('Overfitting Analysis\\n(Train R¬≤ - Test R¬≤)')\n",
    "    axes[1, 0].set_ylabel('Overfitting Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].axhline(y=0.1, color='orange', linestyle='--', label='Overfitting Threshold')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars3, overfitting):\n",
    "        y_pos = bar.get_height() + (0.005 if value >= 0 else -0.01)\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                       f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', \n",
    "                       fontweight='bold')\n",
    "    \n",
    "    # 4. Performance radar chart comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Normalize metrics for radar chart (0-1 scale)\n",
    "    metrics_for_radar = ['Test R¬≤', 'Test RMSE', 'CV R¬≤ Mean'] if 'CV R¬≤ Mean' in final_comparison_df.columns else ['Test R¬≤', 'Test RMSE']\n",
    "    \n",
    "    # For RMSE, we want lower values to be better, so we'll use 1 - normalized RMSE\n",
    "    normalized_data = final_comparison_df.copy()\n",
    "    normalized_data['Test RMSE'] = 1 - (normalized_data['Test RMSE'] - normalized_data['Test RMSE'].min()) / \\\n",
    "                                  (normalized_data['Test RMSE'].max() - normalized_data['Test RMSE'].min())\n",
    "    \n",
    "    # Simple bar chart instead of radar for simplicity\n",
    "    x_pos = range(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar([x - width/2 for x in x_pos], final_comparison_df['Test R¬≤'], \n",
    "           width, label='Test R¬≤', alpha=0.8)\n",
    "    ax4.bar([x + width/2 for x in x_pos], normalized_data['Test RMSE'], \n",
    "           width, label='Normalized RMSE (1-norm)', alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Models')\n",
    "    ax4.set_ylabel('Normalized Score')\n",
    "    ax4.set_title('Overall Performance Comparison')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(models, rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the best model\n",
    "    if best_model_name in trained_models:\n",
    "        best_model = trained_models[best_model_name]\n",
    "        \n",
    "        print(f\"\\nüíæ Saving the best model ({best_model_name})...\")\n",
    "        \n",
    "        # Save model using joblib (more efficient for scikit-learn models)\n",
    "        joblib.dump(best_model, f'best_ghg_emissions_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib')\n",
    "        \n",
    "        # Also save using pickle as backup\n",
    "        with open(f'best_ghg_emissions_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl', 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        \n",
    "        # Save the scaler if Linear Regression was the best model\n",
    "        if 'Linear Regression' in best_model_name and 'scaler' in locals():\n",
    "            joblib.dump(scaler, 'ghg_emissions_scaler.joblib')\n",
    "            print(\"  - Feature scaler saved\")\n",
    "        \n",
    "        # Save feature names and encoders\n",
    "        model_metadata = {\n",
    "            'feature_names': list(X.columns),\n",
    "            'target_name': target_col,\n",
    "            'model_name': best_model_name,\n",
    "            'performance': dict(best_model_performance),\n",
    "            'encoders': encoders if 'encoders' in locals() else None\n",
    "        }\n",
    "        \n",
    "        with open('ghg_emissions_model_metadata.pkl', 'wb') as f:\n",
    "            pickle.dump(model_metadata, f)\n",
    "        \n",
    "        print(\"  - Model metadata saved\")\n",
    "        print(\"‚úÖ Best model and associated files saved successfully!\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéâ GHG EMISSIONS PREDICTION PROJECT COMPLETED! üéâ\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        print(f\"Final Test R¬≤ Score: {best_model_performance['Test R¬≤']:.4f}\")\n",
    "        print(f\"Final Test RMSE: {best_model_performance['Test RMSE']:.4f}\")\n",
    "        print(f\"Dataset Size: {df_final.shape if 'df_final' in locals() else 'N/A'}\")\n",
    "        print(f\"Features Used: {len(X.columns) if 'X' in locals() else 'N/A'}\")\n",
    "        print(\"\\nSaved Files:\")\n",
    "        print(f\"  - best_ghg_emissions_model_{best_model_name.lower().replace(' ', '_')}.joblib\")\n",
    "        print(f\"  - best_ghg_emissions_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "        print(\"  - ghg_emissions_model_metadata.pkl\")\n",
    "        if 'Linear Regression' in best_model_name:\n",
    "            print(\"  - ghg_emissions_scaler.joblib\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Error: Model results not available. Please run all previous steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "# üìä Project Summary and Conclusions\n",
    "\n",
    "## Key Findings:\n",
    "1. **Data Quality**: The dataset contains comprehensive supply chain emission factors for US industries and commodities from 2010-2016.\n",
    "\n",
    "2. **Feature Importance**: The most important features for predicting emission factors include:\n",
    "   - Supply Chain Emission Factors without Margins (baseline)\n",
    "   - Margins of Supply Chain Emission Factors\n",
    "   - Data Quality scores (reliability, temporal, geographical, technological correlations)\n",
    "   - Substance type (CO2, methane, nitrous oxide, other GHGs)\n",
    "\n",
    "3. **Model Performance**: Both Linear Regression and Random Forest models showed good predictive capability, with hyperparameter tuning improving performance.\n",
    "\n",
    "## Recommendations:\n",
    "1. **Data Collection**: Focus on improving data quality scores as they significantly impact prediction accuracy.\n",
    "2. **Feature Engineering**: Consider additional domain-specific features like industry sector groupings.\n",
    "3. **Model Deployment**: The selected model can be used for predicting emission factors for new commodities/industries.\n",
    "\n",
    "## Next Steps:\n",
    "1. **Validation**: Test the model on more recent data (post-2016) if available.\n",
    "2. **Deployment**: Create a web application or API for real-time predictions.\n",
    "3. **Monitoring**: Implement model monitoring to track performance over time.\n",
    "\n",
    "---\n",
    "*This analysis provides insights into greenhouse gas emissions across US supply chains and demonstrates the effectiveness of machine learning in environmental data modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_model_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Using the trained model for predictions\n",
    "if 'best_model' in locals() and 'X_test' in locals():\n",
    "    print(\"üîÆ Model Prediction Demo\")\n",
    "    print(\"\\nMaking predictions on a sample of test data...\")\n",
    "    \n",
    "    # Select a few samples for demonstration\n",
    "    sample_indices = np.random.choice(X_test.index, size=5, replace=False)\n",
    "    sample_X = X_test.loc[sample_indices]\n",
    "    sample_y_true = y_test.loc[sample_indices]\n",
    "    \n",
    "    # Make predictions\n",
    "    if 'Linear Regression' in best_model_name:\n",
    "        sample_X_scaled = pd.DataFrame(scaler.transform(sample_X), \n",
    "                                      columns=sample_X.columns, \n",
    "                                      index=sample_X.index)\n",
    "        sample_predictions = best_model.predict(sample_X_scaled)\n",
    "    else:\n",
    "        sample_predictions = best_model.predict(sample_X)\n",
    "    \n",
    "    # Display results\n",
    "    prediction_results = pd.DataFrame({\n",
    "        'Actual': sample_y_true.values,\n",
    "        'Predicted': sample_predictions,\n",
    "        'Absolute_Error': np.abs(sample_y_true.values - sample_predictions),\n",
    "        'Relative_Error_Pct': (np.abs(sample_y_true.values - sample_predictions) / sample_y_true.values) * 100\n",
    "    }, index=sample_indices)\n",
    "    \n",
    "    print(\"\\nSample Predictions:\")\n",
    "    display(prediction_results.round(4))\n",
    "    \n",
    "    print(f\"\\nAverage Absolute Error on samples: {prediction_results['Absolute_Error'].mean():.4f}\")\n",
    "    print(f\"Average Relative Error on samples: {prediction_results['Relative_Error_Pct'].mean():.2f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model is ready for deployment!\")\n",
    "else:\n",
    "    print(\"Demo not available - please ensure all previous steps completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
