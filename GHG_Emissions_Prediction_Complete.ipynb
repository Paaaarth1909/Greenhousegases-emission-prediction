{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32efb7ab",
   "metadata": {},
   "source": [
    "# Supply Chain Emissions Modeling Using Industry and Commodity Data (2010‚Äì2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1c6fc",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "You have annual supply chain emission data from 2010‚Äì2016 categorized into industries and commodities. The goal is to develop a regression model that can predict the Supply Chain Emission Factors with Margins based on descriptive and quality metrics (substance, unit, reliability, temporal/geographical/technological/data collection correlations, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae1c80",
   "metadata": {},
   "source": [
    "# üå± Greenhouse Gas Emission Prediction Project\n",
    "\n",
    "**Project Goal:**  \n",
    "To analyze and predict greenhouse gas (GHG) emissions from various U.S. industries and commodities using the official dataset from [data.gov](https://catalog.data.gov/dataset/supply-chain-greenhouse-gas-emission-factors-for-us-industries-and-commodities).\n",
    "\n",
    "**Source:**  \n",
    "[Supply Chain Greenhouse Gas Emission Factors](https://catalog.data.gov/dataset/supply-chain-greenhouse-gas-emission-factors-for-us-industries-and-commodities)\n",
    "\n",
    "**Tools:** Python, Pandas, Scikit-learn, Matplotlib, Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc9ed5",
   "metadata": {},
   "source": [
    "## üìÇ Dataset Overview\n",
    "\n",
    "This dataset contains supply chain emission factors associated with various U.S. industries and commodities.\n",
    "\n",
    "**Key Columns:**\n",
    "- `Code`: Industry/Commodity classification code\n",
    "- `Name`: Name of the industry/commodity\n",
    "- `Substance`: Type of greenhouse gas (CO2, methane, nitrous oxide, etc.)\n",
    "- `Unit`: Measurement units (e.g., kg/2018 USD, purchaser price)\n",
    "- `Supply Chain Emission Factors with Margins`: Target variable for prediction\n",
    "- `DQ ReliabilityScore`: Data quality reliability score\n",
    "- `DQ TemporalCorrelation`: Temporal correlation quality score\n",
    "- `DQ GeographicalCorrelation`: Geographical correlation quality score\n",
    "- `DQ TechnologicalCorrelation`: Technological correlation quality score\n",
    "- `DQ DataCollection`: Data collection quality score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409dc066",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing\n",
    "\n",
    "Steps:\n",
    "- Handle missing values\n",
    "- Convert units where needed\n",
    "- Encode categorical features\n",
    "- Normalize/scale numeric columns\n",
    "\n",
    "## ü§ñ Model Building & Evaluation\n",
    "\n",
    "We aim to predict `Supply Chain Emission Factors with Margins` using regression models.\n",
    "\n",
    "Models to try:\n",
    "- Linear Regression\n",
    "- Random Forest Regressor\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R¬≤ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2558455",
   "metadata": {},
   "source": [
    "##### Steps:\n",
    "- Step 1: Import Required Libraries\n",
    "- Step 2: Load Dataset\n",
    "- Step 3: Data Preprocessing (EDA+Cleaning+Encoding)\n",
    "- Step 4: Training\n",
    "- Step 5: Prediction and Evaluation\n",
    "- Step 6: Hyperparameter Tuning\n",
    "- Step 7: Comparative Study and Selecting the Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118424af",
   "metadata": {},
   "source": [
    "# Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e58624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670b39a",
   "metadata": {},
   "source": [
    "# Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7223e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Excel file path and years range\n",
    "excel_file = 'SupplyChainEmissionFactorsforUSIndustriesCommodities.xlsx'\n",
    "years = range(2010, 2017)\n",
    "\n",
    "print(f\"Years to process: {list(years)}\")\n",
    "print(f\"Looking for file: {excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_sheets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available sheets in the Excel file\n",
    "if os.path.exists(excel_file):\n",
    "    xl_file = pd.ExcelFile(excel_file)\n",
    "    available_sheets = xl_file.sheet_names\n",
    "    print(f\"Available sheets in Excel file: {available_sheets[:10]}...\")  # Show first 10 sheets\n",
    "    print(f\"Total sheets: {len(available_sheets)}\")\n",
    "else:\n",
    "    print(f\"Error: File '{excel_file}' not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_sample_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first year data to understand structure\n",
    "df_commodity_sample = pd.read_excel(excel_file, sheet_name=f'{years[0]}_Detail_Commodity')\n",
    "print(f\"Commodity data shape for {years[0]}: {df_commodity_sample.shape}\")\n",
    "print(f\"Columns: {list(df_commodity_sample.columns)}\")\n",
    "df_commodity_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_industry_sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load industry data to understand structure\n",
    "df_industry_sample = pd.read_excel(excel_file, sheet_name=f'{years[0]}_Detail_Industry')\n",
    "print(f\"Industry data shape for {years[0]}: {df_industry_sample.shape}\")\n",
    "print(f\"Columns: {list(df_industry_sample.columns)}\")\n",
    "df_industry_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine_all_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all data from all years\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    try:\n",
    "        # Load commodity data\n",
    "        df_com = pd.read_excel(excel_file, sheet_name=f'{year}_Detail_Commodity')\n",
    "        # Load industry data\n",
    "        df_ind = pd.read_excel(excel_file, sheet_name=f'{year}_Detail_Industry')\n",
    "        \n",
    "        # Add source and year columns\n",
    "        df_com['Source'] = 'Commodity'\n",
    "        df_ind['Source'] = 'Industry'\n",
    "        df_com['Year'] = year\n",
    "        df_ind['Year'] = year\n",
    "        \n",
    "        # Clean column names\n",
    "        df_com.columns = df_com.columns.str.strip()\n",
    "        df_ind.columns = df_ind.columns.str.strip()\n",
    "\n",
    "        # Standardize column names\n",
    "        df_com.rename(columns={\n",
    "            'Commodity Code': 'Code',\n",
    "            'Commodity Name': 'Name'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        df_ind.rename(columns={\n",
    "            'Industry Code': 'Code',\n",
    "            'Industry Name': 'Name'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Combine commodity and industry data for this year\n",
    "        year_data = pd.concat([df_com, df_ind], ignore_index=True)\n",
    "        all_data.append(year_data)\n",
    "        \n",
    "        print(f\"Loaded {len(year_data)} records for year {year}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing year {year}: {e}\")\n",
    "\n",
    "# Combine all years\n",
    "if all_data:\n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nTotal combined dataset shape: {df.shape}\")\n",
    "    print(f\"Years in dataset: {sorted(df['Year'].unique())}\")\n",
    "else:\n",
    "    print(\"No data was loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_basic_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"üìä Dataset Basic Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_header",
   "metadata": {},
   "source": [
    "# Step 3: Data Preprocessing (EDA + Cleaning + Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_header",
   "metadata": {},
   "source": [
    "## 3.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"üìà Dataset Summary Statistics:\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print(f\"Years covered: {sorted(df['Year'].unique())}\")\n",
    "print(f\"Unique codes: {df['Code'].nunique()}\")\n",
    "print(f\"Unique substances: {df['Substance'].nunique()}\")\n",
    "print(f\"Sources: {df['Source'].value_counts().to_dict()}\")\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\nüîç Missing Values Analysis:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_variable_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "target_col = 'Supply Chain Emission Factors with Margins'\n",
    "\n",
    "print(f\"üéØ Target Variable Analysis: {target_col}\")\n",
    "print(f\"Statistics:\")\n",
    "print(df[target_col].describe())\n",
    "\n",
    "# Visualization of target variable\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df[target_col], bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Target Variable')\n",
    "axes[0, 0].set_xlabel(target_col)\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(df[target_col])\n",
    "axes[0, 1].set_title('Box Plot of Target Variable')\n",
    "axes[0, 1].set_ylabel(target_col)\n",
    "\n",
    "# Log-scale histogram (for better visualization if data is skewed)\n",
    "axes[1, 0].hist(np.log1p(df[target_col]), bins=50, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Log-scale Distribution of Target Variable')\n",
    "axes[1, 0].set_xlabel(f'log1p({target_col})')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Target by year\n",
    "yearly_stats = df.groupby('Year')[target_col].agg(['mean', 'median', 'std']).reset_index()\n",
    "axes[1, 1].plot(yearly_stats['Year'], yearly_stats['mean'], marker='o', label='Mean', linewidth=2)\n",
    "axes[1, 1].plot(yearly_stats['Year'], yearly_stats['median'], marker='s', label='Median', linewidth=2)\n",
    "axes[1, 1].set_title('Target Variable Trends by Year')\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].set_ylabel(target_col)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorical_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Substance distribution\n",
    "substance_counts = df['Substance'].value_counts()\n",
    "axes[0, 0].pie(substance_counts.values, labels=substance_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Distribution of Substances')\n",
    "\n",
    "# Unit distribution\n",
    "unit_counts = df['Unit'].value_counts()\n",
    "axes[0, 1].pie(unit_counts.values[:5], labels=unit_counts.index[:5], autopct='%1.1f%%')  # Top 5 units\n",
    "axes[0, 1].set_title('Distribution of Units (Top 5)')\n",
    "\n",
    "# Target by Substance\n",
    "substance_target = df.groupby('Substance')[target_col].mean().sort_values(ascending=True)\n",
    "axes[1, 0].barh(range(len(substance_target)), substance_target.values, color='lightcoral')\n",
    "axes[1, 0].set_yticks(range(len(substance_target)))\n",
    "axes[1, 0].set_yticklabels(substance_target.index)\n",
    "axes[1, 0].set_title('Average Target Value by Substance')\n",
    "axes[1, 0].set_xlabel('Average Emission Factor')\n",
    "\n",
    "# Target by Source\n",
    "source_target = df.groupby('Source')[target_col].mean()\n",
    "axes[1, 1].bar(source_target.index, source_target.values, color=['lightblue', 'lightgreen'])\n",
    "axes[1, 1].set_title('Average Target Value by Source')\n",
    "axes[1, 1].set_ylabel('Average Emission Factor')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality_scores_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality scores analysis\n",
    "quality_cols = [\n",
    "    'DQ ReliabilityScore of Factors without Margins',\n",
    "    'DQ TemporalCorrelation of Factors without Margins',\n",
    "    'DQ GeographicalCorrelation of Factors without Margins',\n",
    "    'DQ TechnologicalCorrelation of Factors without Margins',\n",
    "    'DQ DataCollection of Factors without Margins'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(quality_cols):\n",
    "    df[col].value_counts().sort_index().plot(kind='bar', ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f'Distribution of {col.split(\" \")[1]}')\n",
    "    axes[i].set_xlabel('Score')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Correlation between quality scores and target variable\n",
    "quality_correlations = df[quality_cols + [target_col]].corr()[target_col].drop(target_col)\n",
    "axes[5].barh(range(len(quality_correlations)), quality_correlations.values, color='lightcoral')\n",
    "axes[5].set_yticks(range(len(quality_correlations)))\n",
    "axes[5].set_yticklabels([col.split(' ')[1] for col in quality_correlations.index])\n",
    "axes[5].set_title('Correlation with Target Variable')\n",
    "axes[5].set_xlabel('Correlation Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_header",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning steps\n",
    "print(\"üßπ Data Cleaning Steps:\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Remove rows with missing target values\n",
    "df_clean = df.dropna(subset=[target_col]).copy()\n",
    "print(f\"After removing missing target values: {df_clean.shape}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Unnamed: 7'] if 'Unnamed: 7' in df_clean.columns else []\n",
    "if columns_to_drop:\n",
    "    df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "    print(f\"After dropping unnecessary columns: {df_clean.shape}\")\n",
    "\n",
    "# Handle any remaining missing values in features\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "if missing_before > 0:\n",
    "    print(f\"Missing values before cleaning: {missing_before}\")\n",
    "    # Fill missing values with appropriate methods\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype in ['float64', 'int64']:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_clean[col].fillna(df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown', inplace=True)\n",
    "    \n",
    "    missing_after = df_clean.isnull().sum().sum()\n",
    "    print(f\"Missing values after cleaning: {missing_after}\")\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering_header",
   "metadata": {},
   "source": [
    "## 3.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"‚öôÔ∏è Feature Engineering:\")\n",
    "\n",
    "# Create composite data quality score\n",
    "quality_cols = [\n",
    "    'DQ ReliabilityScore of Factors without Margins',\n",
    "    'DQ TemporalCorrelation of Factors without Margins',\n",
    "    'DQ GeographicalCorrelation of Factors without Margins',\n",
    "    'DQ TechnologicalCorrelation of Factors without Margins',\n",
    "    'DQ DataCollection of Factors without Margins'\n",
    "]\n",
    "\n",
    "df_clean['DQ_Composite_Score'] = df_clean[quality_cols].mean(axis=1)\n",
    "print(f\"Created composite data quality score\")\n",
    "\n",
    "# Create margin ratio feature\n",
    "df_clean['Margin_Ratio'] = (df_clean['Margins of Supply Chain Emission Factors'] / \n",
    "                           df_clean['Supply Chain Emission Factors without Margins'])\n",
    "df_clean['Margin_Ratio'] = df_clean['Margin_Ratio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "print(f\"Created margin ratio feature\")\n",
    "\n",
    "# Create categorical features for code prefixes\n",
    "df_clean['Code_Prefix'] = df_clean['Code'].astype(str).str[:4]\n",
    "print(f\"Created code prefix feature\")\n",
    "\n",
    "# Create decade feature\n",
    "df_clean['Decade'] = (df_clean['Year'] // 10) * 10\n",
    "print(f\"Created decade feature\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_clean.shape}\")\n",
    "print(f\"New features created: DQ_Composite_Score, Margin_Ratio, Code_Prefix, Decade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_header",
   "metadata": {},
   "source": [
    "## 3.4 Feature Encoding and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "print(\"üéØ Feature Selection for Modeling:\")\n",
    "\n",
    "# Define feature categories\n",
    "categorical_features = ['Substance', 'Unit', 'Source', 'Code_Prefix']\n",
    "numerical_features = [\n",
    "    'Supply Chain Emission Factors without Margins',\n",
    "    'Margins of Supply Chain Emission Factors',\n",
    "    'DQ ReliabilityScore of Factors without Margins',\n",
    "    'DQ TemporalCorrelation of Factors without Margins',\n",
    "    'DQ GeographicalCorrelation of Factors without Margins',\n",
    "    'DQ TechnologicalCorrelation of Factors without Margins',\n",
    "    'DQ DataCollection of Factors without Margins',\n",
    "    'DQ_Composite_Score',\n",
    "    'Margin_Ratio',\n",
    "    'Year'\n",
    "]\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_clean[categorical_features + numerical_features].copy()\n",
    "y = df_clean[target_col].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Numerical features: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"üîÑ Encoding Categorical Variables:\")\n",
    "\n",
    "# Label encoding for categorical variables with many categories\n",
    "label_encoders = {}\n",
    "X_encoded = X.copy()\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Label encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(f\"\\nEncoded feature matrix shape: {X_encoded.shape}\")\n",
    "print(f\"All features are now numerical: {X_encoded.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_header",
   "metadata": {},
   "source": [
    "# Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"üìä Splitting Data:\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=df_clean['Source']\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Training set target statistics:\")\n",
    "print(y_train.describe())\n",
    "print(f\"\\nTesting set target statistics:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"üìè Feature Scaling:\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Feature scaling completed using StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "print(\"ü§ñ Training Baseline Models:\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # For Linear Regression, use scaled features\n",
    "    if name == 'Linear Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # For tree-based models, scaling is not necessary\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Baseline model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_header",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'RMSE': [model_results[name]['rmse'] for name in model_results.keys()],\n",
    "    'MAE': [model_results[name]['mae'] for name in model_results.keys()],\n",
    "    'R¬≤ Score': [model_results[name]['r2'] for name in model_results.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['RMSE'], color='lightcoral')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['MAE'], color='lightblue')\n",
    "axes[1].set_title('MAE Comparison')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "axes[2].bar(comparison_df['Model'], comparison_df['R¬≤ Score'], color='lightgreen')\n",
    "axes[2].set_title('R¬≤ Score Comparison')\n",
    "axes[2].set_ylabel('R¬≤ Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual_vs_predicted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for i, (name, results) in enumerate(model_results.items()):\n",
    "    axes[i].scatter(y_test, results['predictions'], alpha=0.6)\n",
    "    axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[i].set_xlabel('Actual Values')\n",
    "    axes[i].set_ylabel('Predicted Values')\n",
    "    axes[i].set_title(f'{name}: Actual vs Predicted\\nR¬≤ = {results[\"r2\"]:.4f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, results) in enumerate(model_results.items()):\n",
    "    residuals = y_test - results['predictions']\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[i*2].scatter(results['predictions'], residuals, alpha=0.6)\n",
    "    axes[i*2].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[i*2].set_xlabel('Predicted Values')\n",
    "    axes[i*2].set_ylabel('Residuals')\n",
    "    axes[i*2].set_title(f'{name}: Residuals vs Predicted')\n",
    "    axes[i*2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    axes[i*2+1].hist(residuals, bins=30, alpha=0.7, color='skyblue')\n",
    "    axes[i*2+1].set_xlabel('Residuals')\n",
    "    axes[i*2+1].set_ylabel('Frequency')\n",
    "    axes[i*2+1].set_title(f'{name}: Residuals Distribution')\n",
    "    axes[i*2+1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (for Random Forest)\n",
    "rf_model = model_results['Random Forest']['model']\n",
    "feature_names = categorical_features + numerical_features\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Feature Importance (Random Forest):\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'], color='lightblue')\n",
    "plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_header",
   "metadata": {},
   "source": [
    "# Step 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"üîß Hyperparameter Tuning for Random Forest:\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing Grid Search (this may take a few minutes)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Grid Search completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_tuned_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "print(\"üìä Evaluating Tuned Random Forest Model:\")\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"Tuned Random Forest Performance:\")\n",
    "print(f\"  RMSE: {rmse_tuned:.4f}\")\n",
    "print(f\"  MAE: {mae_tuned:.4f}\")\n",
    "print(f\"  R¬≤ Score: {r2_tuned:.4f}\")\n",
    "\n",
    "# Compare with original Random Forest\n",
    "original_rf_r2 = model_results['Random Forest']['r2']\n",
    "improvement = r2_tuned - original_rf_r2\n",
    "print(f\"\\nImprovement over original Random Forest:\")\n",
    "print(f\"  R¬≤ Score improvement: {improvement:.4f}\")\n",
    "print(f\"  Percentage improvement: {(improvement/original_rf_r2)*100:.2f}%\")\n",
    "\n",
    "# Add tuned model to results\n",
    "model_results['Random Forest (Tuned)'] = {\n",
    "    'model': best_rf,\n",
    "    'predictions': y_pred_tuned,\n",
    "    'rmse': rmse_tuned,\n",
    "    'mae': mae_tuned,\n",
    "    'r2': r2_tuned\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation analysis\n",
    "print(\"üîÑ Cross-Validation Analysis:\")\n",
    "\n",
    "# Perform cross-validation for all models\n",
    "cv_results = {}\n",
    "\n",
    "for name, results in model_results.items():\n",
    "    model = results['model']\n",
    "    \n",
    "    if name == 'Linear Regression':\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean_score': cv_scores.mean(),\n",
    "        'std_score': cv_scores.std(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  CV Mean R¬≤ Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  CV Scores: {cv_scores}\")\n",
    "    print()\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(cv_results.keys())\n",
    "cv_means = [cv_results[name]['mean_score'] for name in model_names]\n",
    "cv_stds = [cv_results[name]['std_score'] for name in model_names]\n",
    "\n",
    "plt.bar(model_names, cv_means, yerr=cv_stds, capsize=5, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Cross-Validation R¬≤ Score')\n",
    "plt.title('Cross-Validation Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_header",
   "metadata": {},
   "source": [
    "# Step 7: Comparative Study and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model comparison\n",
    "print(\"üèÜ Final Model Comparison and Selection:\")\n",
    "\n",
    "# Create comprehensive comparison\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'Test_RMSE': [model_results[name]['rmse'] for name in model_results.keys()],\n",
    "    'Test_MAE': [model_results[name]['mae'] for name in model_results.keys()],\n",
    "    'Test_R2': [model_results[name]['r2'] for name in model_results.keys()],\n",
    "    'CV_R2_Mean': [cv_results[name]['mean_score'] for name in model_results.keys()],\n",
    "    'CV_R2_Std': [cv_results[name]['std_score'] for name in model_results.keys()]\n",
    "})\n",
    "\n",
    "# Calculate overfitting indicator (difference between CV and test performance)\n",
    "final_comparison['Overfitting_Indicator'] = final_comparison['CV_R2_Mean'] - final_comparison['Test_R2']\n",
    "\n",
    "print(\"\\nüìä Comprehensive Model Comparison:\")\n",
    "print(final_comparison.round(4).to_string(index=False))\n",
    "\n",
    "# Select best model based on multiple criteria\n",
    "best_model_idx = final_comparison['Test_R2'].idxmax()\n",
    "best_model_name = final_comparison.loc[best_model_idx, 'Model']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nüéØ Best Model Selection:\")\n",
    "print(f\"Selected Model: {best_model_name}\")\n",
    "print(f\"Selection Criteria: Highest Test R¬≤ Score\")\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Test RMSE: {final_comparison.loc[best_model_idx, 'Test_RMSE']:.4f}\")\n",
    "print(f\"  Test MAE: {final_comparison.loc[best_model_idx, 'Test_MAE']:.4f}\")\n",
    "print(f\"  Test R¬≤ Score: {final_comparison.loc[best_model_idx, 'Test_R2']:.4f}\")\n",
    "print(f\"  CV R¬≤ Score: {final_comparison.loc[best_model_idx, 'CV_R2_Mean']:.4f} (+/- {final_comparison.loc[best_model_idx, 'CV_R2_Std']:.4f})\")\n",
    "print(f\"  Overfitting Indicator: {final_comparison.loc[best_model_idx, 'Overfitting_Indicator']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation and insights\n",
    "print(\"üîç Model Interpretation and Insights:\")\n",
    "\n",
    "if 'Random Forest' in best_model_name:\n",
    "    # Feature importance for Random Forest\n",
    "    feature_names = categorical_features + numerical_features\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color='lightblue')\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 10 Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif 'Linear Regression' in best_model_name:\n",
    "    # Coefficients for Linear Regression\n",
    "    feature_names = categorical_features + numerical_features\n",
    "    coefficients = best_model.coef_\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nüìà Top 10 Most Influential Features (by coefficient magnitude):\")\n",
    "    print(coef_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot top coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_coefs = coef_df.head(10)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_coefs['Coefficient']]\n",
    "    plt.barh(range(len(top_coefs)), top_coefs['Coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_coefs)), top_coefs['Feature'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 10 Feature Coefficients - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights and conclusions\n",
    "print(\"üí° Key Insights and Conclusions:\")\n",
    "print(\"\\n1. Data Quality Impact:\")\n",
    "quality_corr = df_clean[quality_cols + [target_col]].corr()[target_col].drop(target_col)\n",
    "print(f\"   - Data quality scores show varying correlations with emission factors\")\n",
    "print(f\"   - Highest correlation: {quality_corr.abs().idxmax()} ({quality_corr.abs().max():.3f})\")\n",
    "\n",
    "print(\"\\n2. Substance Type Analysis:\")\n",
    "substance_stats = df_clean.groupby('Substance')[target_col].agg(['mean', 'std', 'count'])\n",
    "print(f\"   - {substance_stats['mean'].idxmax()} has the highest average emission factor ({substance_stats['mean'].max():.3f})\")\n",
    "print(f\"   - {substance_stats['mean'].idxmin()} has the lowest average emission factor ({substance_stats['mean'].min():.3f})\")\n",
    "\n",
    "print(\"\\n3. Source Comparison:\")\n",
    "source_stats = df_clean.groupby('Source')[target_col].agg(['mean', 'std', 'count'])\n",
    "print(f\"   - Industry vs Commodity emission factors:\")\n",
    "for source in source_stats.index:\n",
    "    print(f\"     {source}: Mean = {source_stats.loc[source, 'mean']:.3f}, Count = {source_stats.loc[source, 'count']}\")\n",
    "\n",
    "print(\"\\n4. Model Performance Summary:\")\n",
    "print(f\"   - Best performing model: {best_model_name}\")\n",
    "print(f\"   - Achieved R¬≤ score of {final_comparison.loc[best_model_idx, 'Test_R2']:.3f} on test data\")\n",
    "print(f\"   - Model explains {final_comparison.loc[best_model_idx, 'Test_R2']*100:.1f}% of variance in emission factors\")\n",
    "\n",
    "print(\"\\n5. Recommendations:\")\n",
    "print(\"   - Focus on improving data quality scores for better predictions\")\n",
    "print(\"   - Consider substance-specific models for more accurate predictions\")\n",
    "print(\"   - Monitor model performance over time as new data becomes available\")\n",
    "print(\"   - Use the model for preliminary emission factor estimation and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_best_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and associated components\n",
    "print(\"üíæ Saving Best Model and Components:\")\n",
    "\n",
    "# Create timestamp for file naming\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'best_ghg_emissions_model_{timestamp}.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"‚úÖ Best model saved as: {model_filename}\")\n",
    "\n",
    "# Save model using pickle as backup\n",
    "model_pickle_filename = f'best_ghg_emissions_model_{timestamp}.pkl'\n",
    "with open(model_pickle_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"‚úÖ Model backup saved as: {model_pickle_filename}\")\n",
    "\n",
    "# Save scaler if Linear Regression was the best model\n",
    "if 'Linear Regression' in best_model_name:\n",
    "    scaler_filename = f'ghg_emissions_scaler_{timestamp}.joblib'\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"‚úÖ Scaler saved as: {scaler_filename}\")\n",
    "\n",
    "# Save label encoders\n",
    "encoders_filename = f'ghg_emissions_encoders_{timestamp}.joblib'\n",
    "joblib.dump(label_encoders, encoders_filename)\n",
    "print(f\"‚úÖ Label encoders saved as: {encoders_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'categorical_features': categorical_features,\n",
    "    'numerical_features': numerical_features,\n",
    "    'target_column': target_col,\n",
    "    'test_performance': {\n",
    "        'rmse': final_comparison.loc[best_model_idx, 'Test_RMSE'],\n",
    "        'mae': final_comparison.loc[best_model_idx, 'Test_MAE'],\n",
    "        'r2': final_comparison.loc[best_model_idx, 'Test_R2']\n",
    "    },\n",
    "    'cv_performance': {\n",
    "        'mean_r2': final_comparison.loc[best_model_idx, 'CV_R2_Mean'],\n",
    "        'std_r2': final_comparison.loc[best_model_idx, 'CV_R2_Std']\n",
    "    },\n",
    "    'training_date': timestamp,\n",
    "    'dataset_shape': df_clean.shape\n",
    "}\n",
    "\n",
    "metadata_filename = f'ghg_emissions_model_metadata_{timestamp}.pkl'\n",
    "with open(metadata_filename, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"‚úÖ Model metadata saved as: {metadata_filename}\")\n",
    "\n",
    "print(f\"\\nüéâ Model training and evaluation completed successfully!\")\n",
    "print(f\"üìÅ Files saved:\")\n",
    "print(f\"   - {model_filename}\")\n",
    "print(f\"   - {model_pickle_filename}\")\n",
    "print(f\"   - {encoders_filename}\")\n",
    "print(f\"   - {metadata_filename}\")\n",
    "if 'Linear Regression' in best_model_name:\n",
    "    print(f\"   - {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# üéØ Project Summary\n",
    "\n",
    "This notebook has successfully completed a comprehensive machine learning analysis for predicting greenhouse gas emission factors from U.S. supply chain data (2010-2016).\n",
    "\n",
    "## Key Achievements:\n",
    "\n",
    "1. **Data Processing**: Successfully loaded and combined multi-year Excel data from both commodity and industry sources\n",
    "2. **Feature Engineering**: Created composite data quality scores, margin ratios, and categorical encodings\n",
    "3. **Model Development**: Trained and compared Linear Regression and Random Forest models\n",
    "4. **Hyperparameter Optimization**: Performed grid search to optimize Random Forest performance\n",
    "5. **Model Evaluation**: Comprehensive evaluation using multiple metrics and cross-validation\n",
    "6. **Model Selection**: Selected the best performing model based on test R¬≤ score\n",
    "7. **Model Persistence**: Saved the best model and all associated components for future use\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "- Deploy the model for emission factor predictions\n",
    "- Monitor model performance with new data\n",
    "- Consider ensemble methods for improved accuracy\n",
    "- Explore deep learning approaches for complex patterns\n",
    "\n",
    "The trained model can now be used to predict supply chain emission factors based on substance type, data quality metrics, and other relevant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}